{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Fit-by-rank-difference\" data-toc-modified-id=\"Fit-by-rank-difference-1\">Fit by rank difference</a></span></li><li><span><a href=\"#local-models\" data-toc-modified-id=\"local-models-2\">local models</a></span><ul class=\"toc-item\"><li><span><a href=\"#new\" data-toc-modified-id=\"new-2.1\">new</a></span></li></ul></li><li><span><a href=\"#model-coefficients-by-rank-difference\" data-toc-modified-id=\"model-coefficients-by-rank-difference-3\">model coefficients by rank difference</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Fit-by-rank-difference\" data-toc-modified-id=\"Fit-by-rank-difference-1\">Fit by rank difference</a></span></li><li><span><a href=\"#local-models\" data-toc-modified-id=\"local-models-2\">local models</a></span><ul class=\"toc-item\"><li><span><a href=\"#new\" data-toc-modified-id=\"new-2.1\">new</a></span></li></ul></li><li><span><a href=\"#model-coefficients-by-rank-difference\" data-toc-modified-id=\"model-coefficients-by-rank-difference-3\">model coefficients by rank difference</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i experiments_common.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = pd.read_json('../pref_pair_sets/2019-05-01_uniform-samples_21-axioms.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = traindata.groupby(['query', 'system'])\n",
    "print(len(traindata), 'pairs total')\n",
    "print(g.concordant.count().head(1).values[0], 'pairs per query/system')\n",
    "oneq = traindata.query('query == \"301\" & system == \"DRMM\"')\n",
    "oneq[['rankdiff']].plot.hist(bins=100, title='Distribution of rank distance in training set')\n",
    "plt.gcf().set_facecolor('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*[f'{i:2d}. {j}' for i,j in enumerate(\n",
    "    [x.split('ax_')[1] for x in traindata.columns if 'ax_' in x], 1)], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = !readlink -f ../pref_pair_sets/2019-05-01_uniform-samples_21-axioms.jsonl\n",
    "fn = fn[-1]\n",
    "fn = f'file://{fn}'\n",
    "fn\n",
    "\n",
    "#fn = 'axiomatic-explainability-train-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -I $fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_features = [x for x in df.columns if x.startswith('ax_')]\n",
    "axioms = [x.replace('ax_', '') for x in col_features]\n",
    "systems = list(sorted(x['system'] for x in df.select('system').distinct().collect()))\n",
    "col_target = 'concordant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df[(df.query == '356')&(df.system == 'DRMM')].toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit by rank difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_schema = T.StructType([\n",
    "    T.StructField('query', T.StringType(), False), T.StructField('system', T.StringType(), False),    \n",
    "    T.StructField('rankdiff', T.IntegerType(), False),\n",
    "    T.StructField('prediction', T.ShortType(), False),\n",
    "    T.StructField('truth', T.ShortType(), False),\n",
    "    T.StructField('correct', T.ShortType(), False)\n",
    "]\n",
    ")\n",
    "\n",
    "group_on = ['query', 'system']\n",
    "\n",
    "\n",
    "\n",
    "@pandas_udf(output_schema, PandasUDFType.GROUPED_MAP)\n",
    "def fit_by_rdiff(pdf):\n",
    "    ix = np.array(pdf.index)\n",
    "    np.random.shuffle(ix)\n",
    "    ntest = len(ix)//3\n",
    "    testidx = ix[:ntest]\n",
    "    trainidx = ix[ntest:]\n",
    "    \n",
    "    train = pdf.loc[trainidx]\n",
    "    test = pdf.loc[testidx]\n",
    "    \n",
    "    \n",
    "    penalty = 'l1'\n",
    "    C = 1\n",
    "    \n",
    "    xtr, ytr = train[col_features].values, train[col_target]\n",
    "\n",
    "    cls = sklearn.ensemble.RandomForestClassifier()\n",
    "    #cls = LogisticRegression(solver='saga', penalty=penalty, C=C)\n",
    "    model = cls.fit(xtr, ytr)\n",
    "    \n",
    "    test['prediction'] = model.predict(test[col_features].values)\n",
    "    test['truth'] = test.concordant\n",
    "    test['correct'] = (test.prediction == test.truth).astype(int)\n",
    "        \n",
    "    group_key = list(pdf[group_on].iloc[0])\n",
    "    columns = group_on + ['rankdiff', 'prediction', 'truth', 'correct']\n",
    "\n",
    "    return test[columns]\n",
    "\n",
    "d_fit_rdiff = df.groupBy(['query', 'system']).apply(fit_by_rdiff)\n",
    "dtest = d_fit_rdiff.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = d_fit_rdiff.groupby(['rankdiff'])\n",
    "fit_rdiff_stats = g.agg(sqlf.mean('correct'), sqlf.stddev_samp('correct')).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_rdiff_stats.plot.scatter('rankdiff', 'avg(correct)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# local models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tmp = df\n",
    "\n",
    "group_on = ['query', 'system']\n",
    "\n",
    "\n",
    "output_schema = T.StructType([\n",
    "    T.StructField('query', T.StringType(), False), T.StructField('system', T.StringType(), False),    \n",
    "] + [\n",
    "    T.StructField(a, T.FloatType(), False) for a in col_features\n",
    "] + [\n",
    "    T.StructField('penalty', T.StringType(), False),\n",
    "    T.StructField('C', T.FloatType(), False),\n",
    "    T.StructField('cv_accuracy_mean', T.FloatType(), False),\n",
    "    T.StructField('cv_accuracy_std', T.FloatType(), False),\n",
    "]\n",
    ")\n",
    "\n",
    "@pandas_udf(output_schema, PandasUDFType.GROUPED_MAP)\n",
    "def train_partial_model(pdf):\n",
    "    xs = pdf[col_features].values\n",
    "    ys = pdf[col_target]\n",
    "    group_key = list(pdf[group_on].iloc[0])\n",
    "    columns = group_on + col_features + ['penalty', 'C', 'cv_accuracy_mean', 'cv_accuracy_std']\n",
    "    out = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    for penalty in ['l1']:\n",
    "        for C in [0.01, 0.1, 0.5, 1]:\n",
    "    \n",
    "            cls = LogisticRegression(solver='saga', penalty=penalty, C=C)\n",
    "            scores = cross_val_score(cls, xs, ys, cv=10)\n",
    "            model = cls.fit(xs, ys)\n",
    "\n",
    "\n",
    "            run = pd.DataFrame([\n",
    "                group_key + list(model.coef_.flatten()) + [penalty, C, scores.mean(), scores.std()] ],\n",
    "                columns=columns\n",
    "            )\n",
    "            out = out.append(run, ignore_index=True)\n",
    "    \n",
    "    return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "localmodels = tmp.repartition(900).groupBy(['query', 'system']).apply(train_partial_model).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localmodels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata.sample(n=int(3e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# sample = (df\n",
    "#           #.filter('query == \"427\" and system == \"DRMM\"')\n",
    "#           .toPandas())\n",
    "\n",
    "sample = traindata.sample(n=int(3e5))\n",
    "\n",
    "xs = sample[col_features].values\n",
    "ys = sample[col_target].values\n",
    "\n",
    "gb = sklearn.ensemble.GradientBoostingClassifier(verbose=0,\n",
    "    n_estimators=100)\n",
    "rf = sklearn.ensemble.RandomForestClassifier(verbose=0,\n",
    "    n_estimators=100, max_depth=10)\n",
    "lr = lambda C: LogisticRegression(solver='saga', penalty='l1', C=C)\n",
    "\n",
    "scores = lambda cls: cross_val_score(cls, xs, ys, cv=5)\n",
    "\n",
    "scores_df = pd.DataFrame(\n",
    "    {'rf': scores(rf), 'gb': scores(gb), 'lr_.1': scores(lr(.1)),\n",
    "     'lr_.01': scores(lr(.01)), 'lr_1': scores(lr(1))})\n",
    "scores_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localmodels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localmodels[['cv_accuracy_mean', 'cv_accuracy_std']].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localmodels[['cv_accuracy_mean', 'cv_accuracy_std']].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localmodels.groupby(['system', 'penalty', 'C']).cv_accuracy_mean.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "systems = sorted(localmodels.system.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = localmodels.copy()\n",
    "tmp['reg'] = tmp.C.map(lambda x: f'l1, C={x:.2f}')\n",
    "acc_by_query = (- tmp.groupby('query').cv_accuracy_mean.median()).rank()\n",
    "tmp['q_acc'] = tmp['query'].map(lambda q: f'{int(acc_by_query[q])}.{q}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.barplot(\n",
    "    data=tmp, x='q_acc', y='cv_accuracy_mean', hue='system', \n",
    "    hue_order=systems,\n",
    "    ci='sd', capsize=.1)\n",
    "# plt.gca().set_xticklabels(tmp['query'])\n",
    "plt.ylim(0, 1)\n",
    "plt.grid()\n",
    "plt.title('Explanation accuracy by query (Error bars = sd across different regularization schemes)')\n",
    "plt.gca().set_xticklabels(\n",
    "    [tl._text.split('.')[-1] for tl in plt.gca().get_xticklabels()]\n",
    ")\n",
    "plt.gcf().set_size_inches((40,6))\n",
    "plt.gcf().set_facecolor('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap -- query vs axiom? / per system\n",
    "\n",
    "coeffs = pd.melt(\n",
    "    tmp, id_vars=['system', 'query', 'reg'], \n",
    "    value_vars=col_features, var_name='axiom', value_name='coeff')\n",
    "coeffs['axiom'] = coeffs.axiom.map(lambda n: n.replace('ax_', ''))\n",
    "\n",
    "def draw_heatmap(data, **kwargs):\n",
    "    #print(data)\n",
    "    data = data.pivot('query', 'axiom', 'coeff')[axioms]\n",
    "    annot = data.round(2)\n",
    "    sns.heatmap(data, cmap=sns.cm.vlag_r, vmin=-1, vmax=1, annot=False, cbar=False)\n",
    "    plt.yticks(rotation=0)\n",
    "\n",
    "with sns.plotting_context(font_scale=.75):\n",
    "    g = sns.FacetGrid(coeffs, row='system', col='reg', sharex=True, sharey=True)\n",
    "    g = g.map_dataframe(draw_heatmap)\n",
    "\n",
    "plt.gcf().set_facecolor('white')\n",
    "plt.gcf().set_size_inches((20, 60))\n",
    "plt.gcf().tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplots of coeff distribution\n",
    "\n",
    "g = sns.catplot(data=coeffs, kind='boxen', x='axiom', y='coeff', hue='system', row='reg', hue_order=systems)\n",
    "\n",
    "def adjust(a):\n",
    "    a.grid()\n",
    "    a.axhline()\n",
    "    \n",
    "[adjust(a) for a in g.axes.flatten()]\n",
    "\n",
    "plt.suptitle('Distribution of axiom coefficients in query-local models', y=1.05)\n",
    "plt.gcf().set_facecolor('white')\n",
    "plt.gcf().set_size_inches((20, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "import scipy.spatial.distance as sdist\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "\n",
    "ddata = localmodels.query(\"system == 'DRMM' & C == 0.50\")\n",
    "\n",
    "dmat = sdist.pdist(ddata[col_features], metric=sdist.cosine)\n",
    "dmat = sdist.squareform(dmat)\n",
    "#idx = localmodels[['system', 'query']]\n",
    "#sns.heatmap(dmat)\n",
    "\n",
    "fig = pylab.figure()\n",
    "axdendro = fig.add_axes([0.09,0.1,0.2,0.8])\n",
    "Y = sch.linkage(dmat, method='centroid')\n",
    "Z = sch.dendrogram(Y, orientation='left')\n",
    "axdendro.set_xticks([])\n",
    "axdendro.set_yticks([])\n",
    "\n",
    "axmatrix = fig.add_axes([0.3,0.1,0.6,0.8])\n",
    "index = Z['leaves']\n",
    "D = np.array(dmat)\n",
    "D = D[index,:]\n",
    "D = D[:,index]\n",
    "im = axmatrix.matshow(D, aspect='auto', origin='lower')\n",
    "axmatrix.set_xticklabels(ddata.iloc[index]['query'].values)\n",
    "axmatrix.set_yticks([])\n",
    "\n",
    "axcolor = fig.add_axes([0.91,0.1,0.02,0.8])\n",
    "pylab.colorbar(im, cax=axcolor)\n",
    "\n",
    "fig.set_size_inches((10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from axiomatic.collection.trec import TrecRobustQueries\n",
    "\n",
    "tq = TrecRobustQueries('../robust04/topics.robust04.301-450.601-700.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls ../robust04/topics*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddata.iloc[[1,2]]['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pd.DataFrame(np.dstack(np.unravel_index(dmat.ravel().argsort(0), (dmat.shape)))[0], columns=['x', 'y'])\n",
    "topk = set([tuple(sorted(x)) for x in indices.query('x!=y').head(60).values])\n",
    "\n",
    "for (i, j) in topk:\n",
    "    i, j = ddata.iloc[[i, j]]['query'].values\n",
    "    #i, j = '%03d' %i, '%03d' % j\n",
    "    print(i, j,  '|', tq[i].text, '/', tq[j].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tq._q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddata.iloc[index]['query'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system-wise pca by coeff\n",
    "from sklearn.decomposition import PCA\n",
    "def draw_pca(data, **kwargs):\n",
    "    pca = PCA(n_components=2)\n",
    "    xs = data[col_features]\n",
    "    pca.fit(xs)\n",
    "    xs2 = pca.transform(xs)\n",
    "    plt.scatter(xs2[:, 0], xs2[:, 1])\n",
    "\n",
    "g = sns.FacetGrid(tmp, row='system', col='reg', sharex=True, sharey=True)\n",
    "g = g.map_dataframe(draw_pca)\n",
    "\n",
    "plt.suptitle('Independently fitted PCAs of query-local model coefficients', y=1.03)\n",
    "plt.gcf().set_facecolor('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[tmp.cv_accuracy_mean > .7].plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[tmp.cv_accuracy_mean < .6].plot.scatter('ax_PROX3', 'ax_REG')\n",
    "plt.gcf().set_facecolor('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../src/')\n",
    "from axiomatic.collection.trec import TrecRobustQueries\n",
    "queries = TrecRobustQueries('../robust04/topics.robust04.301-450.601-700.txt')\n",
    "#list(queries._q.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries[401]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query clustering by coeff\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = localmodels.copy()\n",
    "df1 = df1[df1.system=='DRMM']\n",
    "df1.drop(columns=['ax_AND', 'ax_M_AND'], inplace=True)\n",
    "cols = [c for c in df1.columns if 'ax_' in c]\n",
    "df1.head()\n",
    "pd.plotting.radviz(df1[cols + ['system']], 'system')\n",
    "plt.gcf().set_size_inches((10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run(system, penalty, C, sample_weight=None):\n",
    "    res = dict(system=system, penalty=penalty, C=C, sample_weight=sample_weight)\n",
    "\n",
    "    tmp = df[(df['system'] == system)]\n",
    "    cls = LogisticRegression(penalty=penalty, C=C, solver='saga')\n",
    "    xs = tmp[col_features].values\n",
    "    ys = tmp[col_target]\n",
    "\n",
    "    model_key = (system, penalty, C, sample_weight)\n",
    "    \n",
    "    if sample_weight == 'relative_rankdiff':\n",
    "        sample_weight = tmp.rankdiff / tmp.rankdiff.max()\n",
    "    elif sample_weight == 'scorediff':\n",
    "        sample_weight = tmp.scorediff\n",
    "    \n",
    "    model = cls.fit(xs, ys, sample_weight=sample_weight)\n",
    "    \n",
    "    models[model_key] = model\n",
    "    accuracy = model.score(xs, ys, sample_weight=sample_weight)\n",
    "    res['accuracy'] = accuracy\n",
    "\n",
    "    coef = dict(zip(col_features, model.coef_.flatten()))\n",
    "    res.update(coef)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model coefficients by rank difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T13:03:25.742243Z",
     "start_time": "2019-07-24T13:03:25.505373Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/web-search-axiomatic-reranking/axiomatic-explainability/experiments/predict/experiments_common.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbin_edges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmquantiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrankdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_edges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrankdiff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbin_edges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrdiff_bin_edges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbin_edges\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "bin_edges = stats.mstats.mquantiles(df.select(df.rankdiff).toPandas().values, np.linspace(0,1,7))\n",
    "print(bin_edges)\n",
    "plt.hist(dtest.rankdiff, bin_edges, color='w', edgecolor='k')\n",
    "rdiff_bin_edges = bin_edges[1:]\n",
    "\n",
    "@udf\n",
    "def bin_label(rdiff):\n",
    "    ix = rdiff_bin_edges.searchsorted(rdiff)\n",
    "    if ix == 0:\n",
    "        return f'rdiff<={rdiff_bin_edges[0]}'\n",
    "    b = rdiff_bin_edges[max(0, ix-1):ix+1]\n",
    "    return f'{b[0]}<rdiff<={b[1]}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binned = df.withColumn('binlabel', bin_label(df.rankdiff))\n",
    "\n",
    "group_on = ['query', 'system', 'binlabel']\n",
    "\n",
    "output_schema = T.StructType([\n",
    "    T.StructField('query', T.StringType(), False), T.StructField('system', T.StringType(), False),    \n",
    "    T.StructField('binlabel', T.StringType(), False)\n",
    "] + [\n",
    "    T.StructField(a, T.FloatType(), False) for a in col_features\n",
    "] + [\n",
    "    T.StructField('penalty', T.StringType(), False),\n",
    "    T.StructField('C', T.FloatType(), False),\n",
    "    T.StructField('cv_accuracy_mean', T.FloatType(), False),\n",
    "    T.StructField('cv_accuracy_std', T.FloatType(), False),\n",
    "]\n",
    ")\n",
    "\n",
    "@pandas_udf(output_schema, PandasUDFType.GROUPED_MAP)\n",
    "def train_partial_model(pdf):\n",
    "    xs = pdf[col_features].values\n",
    "    ys = pdf[col_target]\n",
    "    group_key = list(pdf[group_on].iloc[0])\n",
    "    columns = group_on + col_features + ['penalty', 'C', 'cv_accuracy_mean', 'cv_accuracy_std']\n",
    "    out = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    for penalty in ['l1']:\n",
    "        for C in [1]:\n",
    "    \n",
    "            cls = LogisticRegression(solver='saga', penalty=penalty, C=C)\n",
    "            scores = cross_val_score(cls, xs, ys, cv=10)\n",
    "            model = cls.fit(xs, ys)\n",
    "\n",
    "\n",
    "            run = pd.DataFrame([\n",
    "                group_key + list(model.coef_.flatten()) + [penalty, C, scores.mean(), scores.std()] ],\n",
    "                columns=columns\n",
    "            )\n",
    "            out = out.append(run, ignore_index=True)\n",
    "    \n",
    "    return out\n",
    "\n",
    "localmodels_binned = df_binned.groupby(group_on).apply(train_partial_model).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binned.filter('query = \"356\"').filter('system = \"DRMM\"').groupby('binlabel').count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binl = list(localmodels_binned.binlabel.unique())\n",
    "binl = sorted(binl, key=lambda x: float(x.split('=')[-1]))\n",
    "localmodels_binned.binlabel = localmodels_binned.binlabel.astype('category')\n",
    "localmodels_binned.binlabel.cat.reorder_categories(binl, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localmodels_binned.groupby(['system', 'binlabel']).cv_accuracy_mean.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localmodels_binned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localmodels_binned.ax_PROX5.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = localmodels_binned.copy()\n",
    "tmp['reg'] = tmp.C.map(lambda x: f'l1, C={x:.2f}')\n",
    "acc_by_query = (- tmp.groupby('query').cv_accuracy_mean.median()).rank()\n",
    "tmp['q_acc'] = tmp['query'].map(lambda q: f'{int(acc_by_query[q])}.{q}')\n",
    "\n",
    "coeffs = pd.melt(\n",
    "    tmp, id_vars=['system', 'query', 'reg', 'binlabel'], \n",
    "    value_vars=col_features, var_name='axiom', value_name='coeff')\n",
    "coeffs['axiom'] = coeffs.axiom.map(lambda n: n.replace('ax_', ''))\n",
    "\n",
    "\n",
    "g = sns.catplot(data=coeffs, kind='boxen', x='axiom', y='coeff', hue='system', row='binlabel', \n",
    "                hue_order=systems, row_order=binl)\n",
    "\n",
    "def adjust(a):\n",
    "    a.grid()\n",
    "    a.axhline()\n",
    "    \n",
    "[adjust(a) for a in g.axes.flatten()]\n",
    "\n",
    "plt.suptitle('Distribution of axiom coefficients in query-local models', y=1.05)\n",
    "plt.gcf().set_facecolor('white')\n",
    "plt.gcf().set_size_inches((20, 42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
