{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user 'pyarrow<=0.9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('../pref_pair_sets/maxrank20-12axioms.csv')\n",
    "df = pd.read_parquet('../pref_pair_sets/sampled_ranks_14_axioms.parquet')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_features = [x for x in df.columns if x.startswith('ax_')]\n",
    "axioms = [x.replace('ax_', '') for x in col_features]\n",
    "systems = list(sorted(df.system.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global\n",
    "Predict pairwise concordance across all of the system's rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_target = 'concordant'\n",
    "\n",
    "models = {}\n",
    "\n",
    "def run(system, penalty, C, sample_weight=None):\n",
    "    res = dict(system=system, penalty=penalty, C=C, sample_weight=sample_weight)\n",
    "\n",
    "    tmp = df[(df['system'] == system)]\n",
    "    cls = LogisticRegression(penalty=penalty, C=C, solver='saga')\n",
    "    xs = tmp[col_features].values\n",
    "    ys = tmp[col_target]\n",
    "\n",
    "    model_key = (system, penalty, C, sample_weight)\n",
    "    \n",
    "    if sample_weight == 'relative_rankdiff':\n",
    "        sample_weight = tmp.rankdiff / tmp.rankdiff.max()\n",
    "    elif sample_weight == 'scorediff':\n",
    "        sample_weight = tmp.scorediff\n",
    "    \n",
    "    model = cls.fit(xs, ys, sample_weight=sample_weight)\n",
    "    \n",
    "    models[model_key] = model\n",
    "    accuracy = model.score(xs, ys, sample_weight=sample_weight)\n",
    "    res['accuracy'] = accuracy\n",
    "\n",
    "    coef = dict(zip(col_features, model.coef_.flatten()))\n",
    "    res.update(coef)\n",
    "    return res\n",
    "\n",
    "penalties = ['l1', 'l2']\n",
    "Cs = [1, 0.5, 0.1, 0.01]\n",
    "weightings = [None, 'relative_rankdiff', 'scorediff']\n",
    "iter_count = len(systems)*len(penalties)*len(weightings)*len(Cs)\n",
    "\n",
    "def all_runs():\n",
    "    for system in systems:\n",
    "        for penalty in penalties:\n",
    "            for C in Cs:\n",
    "                for sample_weight in weightings:\n",
    "                    yield run(system, penalty, C, sample_weight)\n",
    "\n",
    "results = pd.DataFrame(list(tqdm(all_runs(), total=iter_count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordance prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.groupby('system').accuracy.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.groupby(['penalty', 'C']).accuracy.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kendall's tau\n",
    "\n",
    "ktau_denominator = 20*19/2\n",
    "ktau = results.copy()\n",
    "def estimate_tau(r):\n",
    "    k = (r.system, r.penalty, r.C, r.sample_weight)\n",
    "    m = models[k]\n",
    "    samples = df.query(\n",
    "        'system == @r.system & concordant == 1').copy()\n",
    "    xs = samples[col_features].values\n",
    "    ys = samples[target].values\n",
    "    preds = m.predict(xs)\n",
    "    samples['pred'] = preds\n",
    "    ax_scores = samples.groupby(\n",
    "        ['system', 'query', 'upper_score']\n",
    "    ).pred.sum().reset_index().sort_values(\n",
    "        ['system', 'query', 'upper_score'], ascending=True)\n",
    "    tmp = ax_scores.groupby(['system', 'query']).rank(axis=0, method='average', ascending=False)\n",
    "    ax_scores[['pred_rank', 'score_rank']] = tmp\n",
    "    ax_scores['concordant'] = (ax_scores.pred_rank == ax_scores.score_rank)\n",
    "    ax_scores['discordant'] = ~ax_scores.concordant\n",
    "    \n",
    "    tmp = ax_scores.groupby(['system', 'query'])[['concordant', 'discordant']].sum()\n",
    "    tmp['n'] = tmp.concordant + tmp.discordant\n",
    "    tmp['tau'] = (tmp.concordant - tmp.discordant) / (tmp.n * (tmp.n - 1) / 2)\n",
    "    return k, tmp.tau\n",
    "\n",
    "for _, r in tqdm(ktau.iterrows()):\n",
    "    k, tau = estimate_tau(r)\n",
    "    print(k, tau.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Axiom coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head(2), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "coeffs = pd.melt(\n",
    "    results, id_vars=['system', 'sample_weight', 'penalty', 'C'], \n",
    "    value_vars=col_features, var_name='axiom', value_name='coeff')\n",
    "coeffs['axiom'] = coeffs.axiom.map(lambda n: n.replace('ax_', ''))\n",
    "\n",
    "coeffs['reg'] = coeffs.apply(axis=1, func=lambda r: '{}, C={:.2f}'.format(r.penalty, r.C))\n",
    "\n",
    "\n",
    "coeffs['wt'] = coeffs.sample_weight.fillna('none')#.map(lambda s: 'rank' if 'rank' in s else 'score' if 'score' in s else s)\n",
    "del coeffs['sample_weight']\n",
    "\n",
    "#coeffs = coeffs[(coeffs.penalty=='l1')]\n",
    "\n",
    "def draw_heatmap(data, **kwargs):\n",
    "    #print(data)\n",
    "    data = data.pivot('system', 'axiom', 'coeff')[axioms]\n",
    "    annot = data.round(2)\n",
    "    sns.heatmap(data, cmap=sns.cm.vlag_r, annot=annot, vmin=-1, vmax=1, cbar=False)\n",
    "    plt.yticks(rotation=0)\n",
    "\n",
    "with sns.plotting_context(font_scale=.75):\n",
    "    g = sns.FacetGrid(coeffs, col='wt', row='reg', sharex=True, sharey=True)\n",
    "    g = g.map_dataframe(draw_heatmap)\n",
    "\n",
    "plt.gcf().set_size_inches((20, 15))\n",
    "plt.gcf().tight_layout()\n",
    "plt.savefig('heatmaps.png', transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs['axiom'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
